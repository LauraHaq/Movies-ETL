# Movies-ETL
## Overview of Project
A project completed for Amazing Prime to Extract, Transfrom and Load (ETL) three seperate files of Wikipedia data, Kaggle metadata and MovienLens rating data into a PostgresSQL database. 

## Resources
- Data Sources
  - ![movies_metadata.csv](https://github.com/LauraHaq/Movies-ETL/blob/main/Resources/movies_metadata.zip)
  - ![wikiepedia-movies.json](https://github.com/LauraHaq/Movies-ETL/blob/main/Resources/wikipedia-movies.json)
  - ratings.csv (large file)
- Tools
  - Jupyter Notebook
  - pgAdmin4

## Results
1. Files read ![code](https://github.com/LauraHaq/Movies-ETL/blob/main/ETL_function_test.ipynb)
2. Wiki files Extracted and Transformed ![code](https://github.com/LauraHaq/Movies-ETL/blob/main/ETL_clean_wiki_movies.ipynb)
3. Kaggle files Extracted and Transfromed ![code](https://github.com/LauraHaq/Movies-ETL/blob/main/ETL_clean_kaggle_data.ipynb)
4. Database created ![code](https://github.com/LauraHaq/Movies-ETL/blob/main/ETL_create_database.ipynb)

## Summary
The project was a success. Three files with large amount of data was loaded into PostgreSQL database. The Movie database is able to be analyzed and updated as needed by Amazing Prime.
